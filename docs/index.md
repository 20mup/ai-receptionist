# ğŸ¤– AIVA â€“ AI Voice Assistant for Enterprises

**Built during my internship at [Systems Limited](https://www.systemsltd.com/), AIVA is a fully voice-interactive assistant that allows users to ask questions and receive spoken answers about a company â€” powered by Whisper, GPT, FAISS, and ElevenLabs.**

---

## ğŸ“Œ Objective

To create a voice-enabled AI receptionist that could:
- Understand natural spoken language from users
- Search through hundreds of company-specific documents
- Respond with accurate, human-sounding answers
- Be easily adapted to other companies with minimal setup

This project aimed to go beyond static FAQs and deliver a fluid, human-like interaction using generative AI technologies.

---

## ğŸ”§ Technical Breakdown

### ğŸ§  Knowledge Base

I began by scraping **100+ pages** from the Systems Limited website, including:
- Company history, services, locations, leadership bios
- Whitepapers, news, blog posts, and investor info

These pages were cleaned and chunked using LangChain, then embedded into a **FAISS** vector database using OpenAI embeddings.

### ğŸ—£ï¸ Voice Input + Transcription

The app accepts voice input using a custom Streamlit component built with JavaScript.  
Captured audio is passed to **Whisper**, which returns the full transcription.

### ğŸ¤– Query Answering Logic

The transcription is processed through a **LangChain ConversationalRetrievalChain**, which:
- Retrieves relevant chunks from FAISS
- Pipes them into **OpenAI GPT-3.5** for final response generation
- Maintains conversational memory across turns

### ğŸ”Š Voice Output

Final GPT output is sent to **ElevenLabs**, which converts text into fluent speech using a realistic, professional voice.  
This completes the full **voice-to-voice loop**.

---

## ğŸ’» Technologies Used

| Function           | Tool/Library             |
|--------------------|--------------------------|
| Speech Recognition | Whisper (OpenAI)         |
| LLM Integration    | LangChain + GPT-3.5      |
| Data Storage       | FAISS Vector Index       |
| Text-to-Speech     | ElevenLabs               |
| Web UI             | Streamlit                |
| Data Collection    | Playwright (Scraping)    |

---

## ğŸ¯ Key Features

- Fully voice-interactive â€” speak and receive answers
- 100% customizable â€” retrainable on any companyâ€™s knowledge base
- Supports multi-turn memory (context-aware follow-ups)
- Natural, human-like speech via ElevenLabs
- Streamlit frontend with fallback text input

---

## ğŸ’¡ Challenges Faced

- **Chunking web content** effectively for vector search without noise
- Balancing response length vs clarity from GPT
- Handling minor transcription errors from Whisper
- Setting up robust .env variable handling for secure API keys
- Making sure latency stayed low despite multiple APIs in use

---

## ğŸ§  What I Learned

- How to design modular LangChain pipelines (retrieval â†’ memory â†’ generation)
- Deploying real-time Whisper + TTS in a desktop-grade app
- Writing maintainable Python backend code in a short sprint
- Importance of consistent formatting + chunking for retrieval quality

---

## ğŸ“½ï¸ Demo

Watch the full demo recording here:  
ğŸ¥ [Click to View AIVA in Action](https://drive.google.com/file/d/1JInIiivD3RBrqDqMrg24oT3hcPp_cvXB/view)

---

## ğŸ“‚ GitHub + Assets

- [ğŸ”— GitHub Repository](https://github.com/20mup/ai-receptionist)
- [ğŸ¨ Interface Preview](../assets/images/aiva/interface.png)
- [ğŸ“¸ Hero Banner](../assets/images/aiva/hero.png)

---

> _â€œAIVA gave me my first real taste of whatâ€™s possible with multimodal AI â€” and I built it from scratch in 4 weeks.â€_
